# Knowledge Distillation Training Requirements
# Core ML libraries
torch>=2.0.0
transformers>=4.35.0
datasets>=2.14.0
accelerate>=0.24.0

# PEFT for LoRA adapters
peft>=0.6.0

# Training utilities
wandb>=0.15.0
tqdm>=4.64.0

# Data processing
numpy>=1.24.0
pandas>=1.5.0

# Evaluation
scikit-learn>=1.3.0

# Optional: for better performance
flash-attn>=2.3.0  # Optional, for faster attention
