2025-05-30 15:52:50,194 - INFO - Rank: 0, World Size: 2, Local Rank: 0
2025-05-30 15:52:50,284 - INFO - Rank: 1, World Size: 2, Local Rank: 1
2025-05-30 15:52:50,284 - INFO - Loading model: meta-llama/Llama-3.2-1B
2025-05-30 15:52:51,677 - INFO - Loading model: meta-llama/Llama-3.2-1B
2025-05-30 15:53:22,962 - INFO - Applying HQQ quantization...
2025-05-30 15:53:23,086 - INFO - Applying HQQ quantization...
2025-05-30 16:01:49,136 - INFO - Rank: 0, World Size: 2, Local Rank: 0
2025-05-30 16:01:49,244 - INFO - Rank: 1, World Size: 2, Local Rank: 1
2025-05-30 16:01:49,244 - INFO - Loading model: meta-llama/Llama-3.2-1B
2025-05-30 16:01:50,473 - INFO - Loading model: meta-llama/Llama-3.2-1B
2025-05-30 16:01:53,143 - INFO - Applying HQQ quantization...
2025-05-30 16:01:54,702 - INFO - Applying HQQ quantization...
2025-05-30 16:02:11,001 - INFO - Applying LoRA adapters...
2025-05-30 16:02:12,716 - INFO - Applying LoRA adapters...
2025-05-30 16:02:13,266 - INFO - Loading WikiText-v2 dataset...
2025-05-30 16:02:13,267 - INFO - Loading WikiText-v2 dataset...
2025-05-30 16:02:26,649 - INFO - Train dataset size: 23802
2025-05-30 16:02:26,649 - INFO - Validation dataset size: 2461
2025-05-30 16:02:26,650 - ERROR - Training failed with error: TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'
2025-05-30 16:02:26,653 - INFO - Train dataset size: 23802
2025-05-30 16:02:26,653 - INFO - Validation dataset size: 2461
2025-05-30 16:02:26,655 - ERROR - Training failed with error: TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'
2025-05-30 16:05:43,786 - INFO - Rank: 0, World Size: 2, Local Rank: 0
2025-05-30 16:05:43,857 - INFO - Rank: 1, World Size: 2, Local Rank: 1
2025-05-30 16:05:43,857 - INFO - Loading model: meta-llama/Llama-3.2-1B
2025-05-30 16:05:45,098 - INFO - Loading model: meta-llama/Llama-3.2-1B
2025-05-30 16:05:48,847 - INFO - Applying HQQ quantization...
2025-05-30 16:05:50,221 - INFO - Applying HQQ quantization...
2025-05-30 16:06:06,676 - INFO - Applying LoRA adapters...
2025-05-30 16:06:08,312 - INFO - Applying LoRA adapters...
2025-05-30 16:06:08,898 - INFO - Loading WikiText-v2 dataset...
2025-05-30 16:06:08,900 - INFO - Loading WikiText-v2 dataset...
2025-05-30 16:06:20,278 - INFO - Train dataset size: 23802
2025-05-30 16:06:20,278 - INFO - Validation dataset size: 2461
2025-05-30 16:06:20,382 - INFO - Starting training...
2025-05-30 16:06:20,398 - INFO - Train dataset size: 23802
2025-05-30 16:06:20,398 - INFO - Validation dataset size: 2461
2025-05-30 16:06:20,473 - INFO - Starting training...
2025-05-30 16:06:20,571 - ERROR - Training failed with error: No columns in the dataset match the model's forward method signature: ({', '.join(signature_columns)}). The following columns have been ignored: [attention_mask, input_ids, labels, overflow_to_sample_mapping]. Please check the dataset and model. You may need to set `remove_unused_columns=False` in `TrainingArguments`.
2025-05-30 16:06:20,656 - ERROR - Training failed with error: No columns in the dataset match the model's forward method signature: ({', '.join(signature_columns)}). The following columns have been ignored: [attention_mask, input_ids, overflow_to_sample_mapping, labels]. Please check the dataset and model. You may need to set `remove_unused_columns=False` in `TrainingArguments`.
2025-05-30 16:08:03,378 - INFO - Rank: 0, World Size: 2, Local Rank: 0
2025-05-30 16:08:03,378 - INFO - Loading model: meta-llama/Llama-3.2-1B
2025-05-30 16:08:03,454 - INFO - Rank: 1, World Size: 2, Local Rank: 1
2025-05-30 16:08:03,454 - INFO - Loading model: meta-llama/Llama-3.2-1B
2025-05-30 16:08:07,416 - INFO - Applying HQQ quantization...
2025-05-30 16:08:07,850 - INFO - Applying HQQ quantization...
2025-05-30 16:08:25,385 - INFO - Applying LoRA adapters...
2025-05-30 16:08:25,787 - INFO - Applying LoRA adapters...
2025-05-30 16:08:26,318 - INFO - Loading WikiText-v2 dataset...
2025-05-30 16:08:26,319 - INFO - Loading WikiText-v2 dataset...
2025-05-30 16:08:39,000 - INFO - Train dataset size: 23802
2025-05-30 16:08:39,001 - INFO - Validation dataset size: 2461
2025-05-30 16:08:39,009 - INFO - Train dataset size: 23802
2025-05-30 16:08:39,009 - INFO - Validation dataset size: 2461
2025-05-30 16:08:39,107 - INFO - Starting training...
2025-05-30 16:08:39,118 - INFO - Starting training...
2025-05-30 16:08:39,308 - ERROR - Training failed with error: 'DistributedDataParallel' object has no attribute 'gradient_checkpointing_enable'
2025-05-30 16:08:39,323 - ERROR - Training failed with error: 'DistributedDataParallel' object has no attribute 'gradient_checkpointing_enable'
2025-05-30 16:10:08,343 - INFO - Rank: 0, World Size: 2, Local Rank: 0
2025-05-30 16:10:08,344 - INFO - Loading model: meta-llama/Llama-3.2-1B
2025-05-30 16:10:08,451 - INFO - Rank: 1, World Size: 2, Local Rank: 1
2025-05-30 16:10:08,451 - INFO - Loading model: meta-llama/Llama-3.2-1B
2025-05-30 16:10:12,084 - INFO - Applying HQQ quantization...
2025-05-30 16:10:12,302 - INFO - Applying HQQ quantization...
2025-05-30 16:10:30,008 - INFO - Applying LoRA adapters...
2025-05-30 16:10:30,178 - INFO - Applying LoRA adapters...
2025-05-30 16:10:30,709 - INFO - Loading WikiText-v2 dataset...
2025-05-30 16:10:30,710 - INFO - Loading WikiText-v2 dataset...
2025-05-30 16:10:41,806 - INFO - Train dataset size: 23802
2025-05-30 16:10:41,806 - INFO - Validation dataset size: 2461
2025-05-30 16:10:41,881 - INFO - Starting training...
2025-05-30 16:10:42,672 - INFO - Train dataset size: 23802
2025-05-30 16:10:42,673 - INFO - Validation dataset size: 2461
2025-05-30 16:10:42,762 - INFO - Starting training...
2025-05-30 16:10:43,084 - ERROR - Training failed with error: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`labels` in this case) have excessive nesting (inputs type `list` where type `int` is expected).
2025-05-30 16:10:44,484 - ERROR - Training failed with error: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`labels` in this case) have excessive nesting (inputs type `list` where type `int` is expected).
2025-05-30 19:25:09,448 - INFO - Rank: 0, World Size: 2, Local Rank: 0
2025-05-30 19:25:09,448 - INFO - Loading model: meta-llama/Llama-3.2-1B
2025-05-30 19:25:09,550 - INFO - Rank: 1, World Size: 2, Local Rank: 1
2025-05-30 19:25:09,551 - INFO - Loading model: meta-llama/Llama-3.2-1B
2025-05-30 19:25:13,898 - INFO - Applying HQQ quantization...
2025-05-30 19:25:15,976 - INFO - Applying HQQ quantization...
2025-05-30 19:25:31,926 - INFO - Applying LoRA adapters...
2025-05-30 19:25:34,193 - INFO - Applying LoRA adapters...
2025-05-30 19:25:34,733 - INFO - Loading WikiText-v2 dataset...
2025-05-30 19:25:34,734 - INFO - Loading WikiText-v2 dataset...
2025-05-30 19:25:54,582 - INFO - Train dataset size: 4968
2025-05-30 19:25:54,582 - INFO - Validation dataset size: 530
2025-05-30 19:25:54,590 - INFO - Train dataset size: 4968
2025-05-30 19:25:54,590 - INFO - Validation dataset size: 530
2025-05-30 19:25:54,653 - INFO - Starting training...
2025-05-30 19:25:54,679 - INFO - Starting training...
2025-05-30 19:25:55,029 - ERROR - Training failed with error: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`labels` in this case) have excessive nesting (inputs type `list` where type `int` is expected).
2025-05-30 19:25:56,386 - ERROR - Training failed with error: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`labels` in this case) have excessive nesting (inputs type `list` where type `int` is expected).
2025-05-30 19:27:45,753 - INFO - Rank: 0, World Size: 2, Local Rank: 0
2025-05-30 19:27:45,754 - INFO - Loading model: meta-llama/Llama-3.2-1B
2025-05-30 19:27:45,838 - INFO - Rank: 1, World Size: 2, Local Rank: 1
2025-05-30 19:27:45,838 - INFO - Loading model: meta-llama/Llama-3.2-1B
2025-05-30 19:27:49,811 - INFO - Applying HQQ quantization...
2025-05-30 19:27:49,888 - INFO - Applying HQQ quantization...
2025-05-30 19:28:07,653 - INFO - Applying LoRA adapters...
2025-05-30 19:28:07,939 - INFO - Applying LoRA adapters...
2025-05-30 19:28:08,468 - INFO - Loading WikiText-v2 dataset...
2025-05-30 19:28:08,469 - INFO - Loading WikiText-v2 dataset...
2025-05-30 19:28:16,706 - INFO - Tokenizing train dataset...
2025-05-30 19:28:16,803 - INFO - Tokenizing train dataset...
2025-05-30 19:28:25,257 - INFO - Tokenizing validation dataset...
2025-05-30 19:28:25,324 - INFO - Tokenizing validation dataset...
2025-05-30 19:28:56,774 - INFO - Train dataset size: 18837
2025-05-30 19:28:56,775 - INFO - Validation dataset size: 2015
2025-05-30 19:28:56,775 - INFO - Sample input_ids length: 512
2025-05-30 19:28:56,776 - INFO - Sample attention_mask length: 512
2025-05-30 19:28:56,776 - INFO - Sample labels length: 512
2025-05-30 19:28:56,867 - INFO - Starting training...
2025-05-30 19:28:57,347 - INFO - Train dataset size: 18837
2025-05-30 19:28:57,347 - INFO - Validation dataset size: 2015
2025-05-30 19:28:57,348 - INFO - Sample input_ids length: 512
2025-05-30 19:28:57,348 - INFO - Sample attention_mask length: 512
2025-05-30 19:28:57,348 - INFO - Sample labels length: 512
2025-05-30 19:28:57,433 - INFO - Starting training...
2025-05-30 19:29:08,978 - ERROR - Training failed with error: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-30 19:29:09,201 - ERROR - Training failed with error: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-30 19:31:19,440 - INFO - Rank: 0, World Size: 2, Local Rank: 0
2025-05-30 19:31:19,440 - INFO - Loading model: meta-llama/Llama-3.2-1B
2025-05-30 19:31:19,534 - INFO - Rank: 1, World Size: 2, Local Rank: 1
2025-05-30 19:31:19,534 - INFO - Loading model: meta-llama/Llama-3.2-1B
2025-05-30 19:31:23,413 - INFO - Applying HQQ quantization...
2025-05-30 19:31:23,687 - INFO - Applying HQQ quantization...
2025-05-30 19:31:41,469 - INFO - Applying LoRA adapters...
2025-05-30 19:31:41,676 - INFO - Total trainable parameters: 11,272,192
2025-05-30 19:31:41,681 - INFO - Trainable parameters after GPU move: 11,272,192
2025-05-30 19:31:41,686 - INFO - Applying LoRA adapters...
2025-05-30 19:31:41,889 - INFO - Total trainable parameters: 11,272,192
2025-05-30 19:31:41,894 - INFO - Trainable parameters after GPU move: 11,272,192
2025-05-30 19:31:42,289 - INFO - Loading WikiText-v2 dataset...
2025-05-30 19:31:42,290 - INFO - Loading WikiText-v2 dataset...
2025-05-30 19:31:50,617 - INFO - Tokenizing train dataset...
2025-05-30 19:31:50,672 - INFO - Tokenizing train dataset...
2025-05-30 19:31:59,142 - INFO - Tokenizing validation dataset...
2025-05-30 19:31:59,327 - INFO - Tokenizing validation dataset...
2025-05-30 19:32:31,228 - INFO - Train dataset size: 18837
2025-05-30 19:32:31,228 - INFO - Validation dataset size: 2015
2025-05-30 19:32:31,229 - INFO - Sample input_ids length: 512
2025-05-30 19:32:31,229 - INFO - Sample attention_mask length: 512
2025-05-30 19:32:31,229 - INFO - Sample labels length: 512
2025-05-30 19:32:31,316 - INFO - Starting training...
2025-05-30 19:32:31,472 - INFO - Train dataset size: 18837
2025-05-30 19:32:31,472 - INFO - Validation dataset size: 2015
2025-05-30 19:32:31,473 - INFO - Sample input_ids length: 512
2025-05-30 19:32:31,473 - INFO - Sample attention_mask length: 512
2025-05-30 19:32:31,473 - INFO - Sample labels length: 512
2025-05-30 19:32:31,577 - INFO - Starting training...
2025-05-30 19:32:35,852 - ERROR - Training failed with error: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-30 19:32:36,020 - ERROR - Training failed with error: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-30 19:35:47,114 - INFO - Rank: 0, World Size: 2, Local Rank: 0
2025-05-30 19:35:47,114 - INFO - Loading model: meta-llama/Llama-3.2-1B
2025-05-30 19:35:47,185 - INFO - Rank: 1, World Size: 2, Local Rank: 1
2025-05-30 19:35:47,185 - INFO - Loading model: meta-llama/Llama-3.2-1B
2025-05-30 19:35:52,023 - INFO - Applying LoRA adapters...
2025-05-30 19:35:52,144 - INFO - Applying LoRA adapters...
2025-05-30 19:35:52,282 - INFO - Applying HQQ quantization to base model...
2025-05-30 19:35:52,396 - INFO - Applying HQQ quantization to base model...
2025-05-30 19:36:10,994 - INFO - Total trainable parameters after quant+LoRA: 0
2025-05-30 19:36:10,995 - ERROR - No trainable parameters detected after quant+LoRA!
2025-05-30 19:36:10,995 - ERROR - Training failed with error: LoRA parameters were not set as trainable.
2025-05-30 19:36:11,000 - INFO - Total trainable parameters after quant+LoRA: 0
2025-05-30 19:36:11,001 - ERROR - No trainable parameters detected after quant+LoRA!
2025-05-30 19:36:11,001 - ERROR - Training failed with error: LoRA parameters were not set as trainable.
2025-05-30 19:39:32,886 - INFO - Rank: 0, World Size: 2, Local Rank: 0
2025-05-30 19:39:32,886 - INFO - Loading model: meta-llama/Llama-3.2-1B
2025-05-30 19:39:33,019 - INFO - Rank: 1, World Size: 2, Local Rank: 1
2025-05-30 19:39:33,019 - INFO - Loading model: meta-llama/Llama-3.2-1B
2025-05-30 19:39:36,687 - INFO - Applying HQQ quantization to base model...
2025-05-30 19:39:37,494 - INFO - Applying HQQ quantization to base model...
2025-05-30 19:39:54,505 - INFO - Applying LoRA adapters...
2025-05-30 19:39:54,711 - INFO - Total trainable LoRA parameters: 11,272,192
2025-05-30 19:39:54,717 - INFO - Trainable parameters after GPU move: 11,272,192
2025-05-30 19:39:55,507 - INFO - Applying LoRA adapters...
2025-05-30 19:39:55,705 - INFO - Total trainable LoRA parameters: 11,272,192
2025-05-30 19:39:55,711 - INFO - Trainable parameters after GPU move: 11,272,192
2025-05-30 19:39:56,042 - INFO - Loading WikiText-v2 dataset...
2025-05-30 19:39:56,042 - INFO - Loading WikiText-v2 dataset...
2025-05-30 19:40:03,600 - INFO - Tokenizing train dataset...
2025-05-30 19:40:04,281 - INFO - Tokenizing train dataset...
2025-05-30 19:40:12,152 - INFO - Tokenizing validation dataset...
2025-05-30 19:40:12,836 - INFO - Tokenizing validation dataset...
2025-05-30 19:40:43,712 - INFO - Train dataset size: 18837
2025-05-30 19:40:43,712 - INFO - Validation dataset size: 2015
2025-05-30 19:40:43,713 - INFO - Sample input_ids length: 512
2025-05-30 19:40:43,713 - INFO - Sample attention_mask length: 512
2025-05-30 19:40:43,713 - INFO - Sample labels length: 512
2025-05-30 19:40:43,806 - INFO - Starting training...
2025-05-30 19:40:44,790 - INFO - Train dataset size: 18837
2025-05-30 19:40:44,790 - INFO - Validation dataset size: 2015
2025-05-30 19:40:44,791 - INFO - Sample input_ids length: 512
2025-05-30 19:40:44,791 - INFO - Sample attention_mask length: 512
2025-05-30 19:40:44,791 - INFO - Sample labels length: 512
2025-05-30 19:40:44,880 - INFO - Starting training...
2025-05-30 19:40:49,086 - ERROR - Training failed with error: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-30 19:40:49,174 - ERROR - Training failed with error: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-30 19:47:34,905 - INFO - Rank: 0, World Size: 2, Local Rank: 0
2025-05-30 19:47:34,905 - INFO - Loading model: meta-llama/Llama-3.2-1B
2025-05-30 19:47:35,029 - INFO - Rank: 1, World Size: 2, Local Rank: 1
2025-05-30 19:47:35,029 - INFO - Loading model: meta-llama/Llama-3.2-1B
2025-05-30 19:47:38,891 - INFO - Applying HQQ quantization to base model...
2025-05-30 19:47:39,841 - INFO - Applying HQQ quantization to base model...
2025-05-30 19:47:56,605 - INFO - Applying LoRA adapters...
2025-05-30 19:47:56,810 - INFO - Total trainable LoRA parameters: 11,272,192
2025-05-30 19:47:56,819 - INFO - Trainable parameters after GPU move: 11,272,192
2025-05-30 19:47:57,774 - INFO - Applying LoRA adapters...
2025-05-30 19:47:57,972 - INFO - Total trainable LoRA parameters: 11,272,192
2025-05-30 19:47:57,979 - INFO - Trainable parameters after GPU move: 11,272,192
2025-05-30 19:47:58,313 - INFO - Loading WikiText-v2 dataset...
2025-05-30 19:47:58,314 - INFO - Loading WikiText-v2 dataset...
2025-05-30 19:48:06,342 - INFO - Tokenizing train dataset...
2025-05-30 19:48:07,406 - INFO - Tokenizing train dataset...
2025-05-30 19:48:14,904 - INFO - Tokenizing validation dataset...
2025-05-30 19:48:15,943 - INFO - Tokenizing validation dataset...
2025-05-30 19:48:46,854 - INFO - Train dataset size: 18837
2025-05-30 19:48:46,854 - INFO - Validation dataset size: 2015
2025-05-30 19:48:46,855 - INFO - Sample input_ids length: 512
2025-05-30 19:48:46,855 - INFO - Sample attention_mask length: 512
2025-05-30 19:48:46,855 - INFO - Sample labels length: 512
2025-05-30 19:48:46,948 - INFO - Starting training...
2025-05-30 19:48:48,146 - INFO - Train dataset size: 18837
2025-05-30 19:48:48,146 - INFO - Validation dataset size: 2015
2025-05-30 19:48:48,147 - INFO - Sample input_ids length: 512
2025-05-30 19:48:48,147 - INFO - Sample attention_mask length: 512
2025-05-30 19:48:48,147 - INFO - Sample labels length: 512
2025-05-30 19:48:48,259 - INFO - Starting training...
2025-05-30 19:48:52,368 - ERROR - Training failed with error: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-30 19:48:52,408 - ERROR - Training failed with error: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-31 14:54:46,119 - INFO - Rank: 0, World Size: 2, Local Rank: 0
2025-05-31 14:54:46,119 - INFO - Loading model: meta-llama/Llama-3.2-1B
2025-05-31 14:54:46,174 - INFO - Rank: 1, World Size: 2, Local Rank: 1
2025-05-31 14:54:46,174 - INFO - Loading model: meta-llama/Llama-3.2-1B
2025-05-31 14:54:51,877 - INFO - Applying HQQ quantization to base model...
2025-05-31 14:54:52,441 - INFO - Applying HQQ quantization to base model...
2025-05-31 14:55:10,072 - INFO - Applying LoRA adapters...
2025-05-31 14:55:10,278 - INFO - Total trainable LoRA parameters: 11,272,192
2025-05-31 14:55:10,285 - INFO - Trainable parameters after GPU move: 11,272,192
2025-05-31 14:55:10,679 - INFO - Applying LoRA adapters...
2025-05-31 14:55:10,888 - INFO - Total trainable LoRA parameters: 11,272,192
2025-05-31 14:55:10,894 - INFO - Trainable parameters after GPU move: 11,272,192
2025-05-31 14:55:11,292 - INFO - Loading WikiText-v2 dataset...
2025-05-31 14:55:11,292 - INFO - Loading WikiText-v2 dataset...
2025-05-31 14:55:29,070 - INFO - Tokenizing train dataset...
2025-05-31 14:55:29,485 - INFO - Tokenizing train dataset...
2025-05-31 14:55:37,690 - INFO - Tokenizing validation dataset...
2025-05-31 14:55:37,982 - INFO - Tokenizing validation dataset...
2025-05-31 14:56:09,876 - INFO - Train dataset size: 18837
2025-05-31 14:56:09,876 - INFO - Validation dataset size: 2015
2025-05-31 14:56:09,877 - INFO - Sample input_ids length: 512
2025-05-31 14:56:09,877 - INFO - Sample attention_mask length: 512
2025-05-31 14:56:09,877 - INFO - Sample labels length: 512
2025-05-31 14:56:09,971 - INFO - Starting training...
2025-05-31 14:56:10,885 - INFO - Train dataset size: 18837
2025-05-31 14:56:10,885 - INFO - Validation dataset size: 2015
2025-05-31 14:56:10,886 - INFO - Sample input_ids length: 512
2025-05-31 14:56:10,886 - INFO - Sample attention_mask length: 512
2025-05-31 14:56:10,886 - INFO - Sample labels length: 512
2025-05-31 14:56:10,982 - INFO - Starting training...
2025-05-31 14:56:15,215 - ERROR - Training failed with error: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-31 14:56:15,276 - ERROR - Training failed with error: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-31 14:58:57,349 - INFO - Rank: 0, World Size: 2, Local Rank: 0
2025-05-31 14:58:57,349 - INFO - Loading model: meta-llama/Llama-3.2-1B
2025-05-31 14:58:57,439 - INFO - Rank: 1, World Size: 2, Local Rank: 1
2025-05-31 14:58:57,439 - INFO - Loading model: meta-llama/Llama-3.2-1B
2025-05-31 14:59:03,002 - INFO - Applying HQQ quantization to base model...
2025-05-31 14:59:03,375 - INFO - Applying HQQ quantization to base model...
2025-05-31 14:59:20,875 - INFO - Applying LoRA adapters...
2025-05-31 14:59:21,080 - INFO - Total trainable LoRA parameters: 11,272,192
2025-05-31 14:59:21,086 - INFO - Trainable parameters after GPU move: 11,272,192
2025-05-31 14:59:21,341 - INFO - Applying LoRA adapters...
2025-05-31 14:59:21,540 - INFO - Total trainable LoRA parameters: 11,272,192
2025-05-31 14:59:21,546 - INFO - Trainable parameters after GPU move: 11,272,192
2025-05-31 14:59:21,880 - INFO - Loading WikiText-v2 dataset...
2025-05-31 14:59:21,885 - INFO - Loading WikiText-v2 dataset...
2025-05-31 14:59:40,419 - INFO - Tokenizing train dataset...
2025-05-31 14:59:40,494 - INFO - Tokenizing train dataset...
2025-05-31 14:59:48,968 - INFO - Tokenizing validation dataset...
2025-05-31 14:59:49,016 - INFO - Tokenizing validation dataset...
2025-05-31 15:00:20,816 - INFO - Train dataset size: 18837
2025-05-31 15:00:20,816 - INFO - Validation dataset size: 2015
2025-05-31 15:00:20,817 - INFO - Sample input_ids length: 512
2025-05-31 15:00:20,817 - INFO - Sample attention_mask length: 512
2025-05-31 15:00:20,817 - INFO - Sample labels length: 512
2025-05-31 15:00:20,938 - INFO - Starting training...
2025-05-31 15:00:21,071 - INFO - Train dataset size: 18837
2025-05-31 15:00:21,071 - INFO - Validation dataset size: 2015
2025-05-31 15:00:21,072 - INFO - Sample input_ids length: 512
2025-05-31 15:00:21,072 - INFO - Sample attention_mask length: 512
2025-05-31 15:00:21,072 - INFO - Sample labels length: 512
2025-05-31 15:00:21,157 - INFO - Starting training...
